ğŸ“˜ Q&A Chatbot using LangChain, FAISS & Ollama

This project implements a PDF-based Question-Answering Chatbot built using:

LangChain (LLM orchestration)

FAISS (vector database for semantic search)

Ollama (local LLM inference)

Embeddings-based document retrieval

The chatbot allows users to upload PDF files, process them into embeddings, store them locally, and then ask natural-language questions about the content.

ğŸš€ Features

âœ” Load and process PDF documents
âœ” Extract text and create embeddings
âœ” Store vectors in FAISS for fast retrieval
âœ” Retrieve context based on user queries
âœ” Generate answers using an Ollama LLM model
âœ” Runs fully locally, no external API required

ğŸ§  How It Works

Load PDF â†’ Text extraction from pages

Chunk & Embed â†’ Generate semantic embeddings

Store in FAISS â†’ Vector index for similarity search

User Query â†’ Embed + search nearest chunks

Ollama LLM â†’ Generate final response based on retrieved context

ğŸ“ Project Structure
.
â”œâ”€â”€ QAChatbot.ipynb           # Main notebook
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ data/                     # (Optional) PDF files

ğŸ›  Installation

Create an environment and install dependencies:

pip install -r requirements.txt


Make sure Ollama is installed and running:

Install: https://ollama.com/download

Pull a model (example):

ollama pull phi

â–¶ï¸ Running the Notebook

Start Jupyter:

jupyter notebook


Open QAChatbot.ipynb and run all cells.

ğŸ“„ Requirements

The project uses the following key libraries:

langchain

langchain-community

langchain-core

langchain-ollama

faiss-cpu

python-dotenv

Full list is in requirements.txt.

ğŸ“š Example Workflow

Place your PDF into the data/ folder.

Run the notebook/script.

The system loads the PDF, creates embeddings, and builds a FAISS index.

Ask any question â€” the chatbot finds relevant chunks and generates an answer.

ğŸ§© Customization

You can modify:

The PDF loader

Chunk size

Embedding model

Ollama model version

Retrieval strategy

ğŸ›¡ Notes

No API keys are required unless you swap Ollama for an API-based LLM.

Ensure .env is excluded using .gitignore.

For large PDFs, embedding might take more time.

ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
