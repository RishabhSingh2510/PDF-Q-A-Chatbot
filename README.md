# ğŸ“˜ Q&A Chatbot using LangChain, FAISS & Ollama

This project implements a **PDF-based Question-Answering** Chatbot built using:

LangChain (LLM orchestration)

FAISS (vector database for semantic search)

Ollama (local LLM inference)

Embeddings-based document retrieval

The chatbot allows users to upload PDF files, process them into embeddings, store them locally, and then ask natural-language questions about the content.

## ğŸš€ Features

âœ” Load and process PDF documents

âœ” Extract text and create embeddings

âœ” Store vectors in FAISS for fast retrieval

âœ” Retrieve context based on user queries

âœ” Generate answers using an Ollama LLM model

âœ” Runs fully locally, no external API required

## ğŸ§  How It Works

Load PDF â†’ Text extraction from pages

Chunk & Embed â†’ Generate semantic embeddings

Store in FAISS â†’ Vector index for similarity search

User Query â†’ Embed + search nearest chunks

Ollama LLM â†’ Generate final response based on retrieved context

## ğŸ“ Project Structure
```
â”œâ”€â”€ QAChatbot.ipynb           # Main notebook
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ data/                     # PDF files
```

## ğŸ›  Installation

Install dependencies:

`pip install -r requirements.txt`


## Install and run Ollama:
https://ollama.com/download

Pull a model (example):

`ollama pull phi`

## â–¶ï¸ Running the Notebook

Start Jupyter:

`jupyter notebook`


Open QAChatbot.ipynb and run all cells.

## ğŸ“„ Requirements

### Key libraries:

langchain

langchain-community

langchain-core

langchain-ollama

faiss-cpu

python-dotenv

Full list is in requirements.txt.

## ğŸ“š Example Workflow

Place your PDF into the data/ folder.

Run the notebook or script.

The system loads the PDF, creates embeddings, and builds a FAISS index.

Ask any question â€” the chatbot retrieves relevant chunks and generates an answer.

## ğŸ§© Customization

### You can modify:

PDF loader

Chunk size

Embedding model

Ollama model version

Retrieval strategy

## ğŸ›¡ Notes

No API keys are required unless you swap Ollama for an API-based LLM.

Embedding may take longer for large PDFs.

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
